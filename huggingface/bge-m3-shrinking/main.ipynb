{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "730d9734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9e40b52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, XLMRobertaTokenizerFast\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from transformers import XLMRobertaTokenizerFast\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"sentencepiece/python/src/sentencepiece\")\n",
    "import sentencepiece_model_pb2 as pb2_model\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e62d3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = 'BAAI/bge-m3'\n",
    "vocab_orig_path = f\"bge-m3_en_ru\"\n",
    "vocab_short_path = f\"short_model/spiece-short.model\"\n",
    "\n",
    "\n",
    "base_tokenizer = XLMRobertaTokenizerFast.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c5020",
   "metadata": {},
   "source": [
    "https://translate.yandex.ru/corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08e91805",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = ''\n",
    "\n",
    "df_en = pd.read_csv(corpus_path + 'corpus.en_ru.1m.en', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
    "df_ru = pd.read_csv(corpus_path + 'corpus.en_ru.1m.ru', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
    "df_en.columns = ['text']\n",
    "df_ru.columns = ['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86f06820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 1000000/1000000 [03:19<00:00, 5015.07it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 1000000/1000000 [03:12<00:00, 5182.02it/s]\n"
     ]
    }
   ],
   "source": [
    "cnt_ru = Counter()\n",
    "for text in tqdm(df_ru.text):\n",
    "    cnt_ru.update(base_tokenizer(text)['input_ids'])\n",
    "    \n",
    "cnt_en = Counter()\n",
    "for text in tqdm(df_en.text):\n",
    "    cnt_en.update(base_tokenizer(text)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56003886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46166\n"
     ]
    }
   ],
   "source": [
    "resulting_vocab = {\n",
    "    base_tokenizer.vocab[k] for k in base_tokenizer.special_tokens_map.values()\n",
    "}\n",
    "for k, v in cnt_ru.items():\n",
    "    if v >= 3 or k <= 100:\n",
    "        resulting_vocab.add(k)\n",
    "for k, v in cnt_en.items():\n",
    "    if v >= 10 or k <= 100:\n",
    "        resulting_vocab.add(k)\n",
    "\n",
    "resulting_vocab = sorted(resulting_vocab)\n",
    "print(len(resulting_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c431736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_voc = {idx: word for word, idx in base_tokenizer.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c8c7d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.txt', 'w', encoding='utf-8') as f:\n",
    "    for idx in resulting_vocab:\n",
    "        f.write(inv_voc[idx] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03fa0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e4f138c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:836: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('bge-m3_en_ru/tokenizer_config.json',\n",
       " 'bge-m3_en_ru/special_tokens_map.json',\n",
       " 'bge-m3_en_ru/sentencepiece.bpe.model',\n",
       " 'bge-m3_en_ru/added_tokens.json',\n",
       " 'bge-m3_en_ru/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_orig = AutoModel.from_pretrained(base_model)\n",
    "tokenizer_orig = XLMRobertaTokenizerFast.from_pretrained(base_model)\n",
    "tokenizer_orig.save_pretrained(vocab_orig_path)\n",
    "\n",
    "with open(vocab_orig_path + '/sentencepiece.bpe.model', 'rb') as f:\n",
    "    data = f.read()\n",
    "m = pb2_model.ModelProto()\n",
    "m.ParseFromString(data)\n",
    "\n",
    "for i in range(250000, 0, -1):\n",
    "    if i not in resulting_vocab:\n",
    "        _ = m.pieces.pop(i - 1)\n",
    "    \n",
    "with open(vocab_short_path, 'wb') as f: f.write(m.SerializeToString())\n",
    "m = None\n",
    "\n",
    "tokenizer_fast_tiny = XLMRobertaTokenizerFast(vocab_file=vocab_short_path)\n",
    "tokenizer_fast_tiny.save_pretrained(vocab_orig_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d6f61ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46166"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer = XLMRobertaTokenizerFast.from_pretrained(vocab_orig_path)\n",
    "new_size = len(new_tokenizer)\n",
    "new_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8642df56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name BAAI/bge-m3_en_ru. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "e = torch.nn.modules.sparse.Embedding(\n",
    "    num_embeddings=len(resulting_vocab),\n",
    "    embedding_dim=1024,\n",
    "    padding_idx=0,\n",
    "    _weight=model_orig.embeddings.word_embeddings.weight.data[resulting_vocab, :]\n",
    ")\n",
    "\n",
    "small_model = AutoModel.from_pretrained(base_model)\n",
    "small_model.config.vocab_size = new_size\n",
    "small_model.set_input_embeddings(e)\n",
    "small_model.tie_weights()\n",
    "\n",
    "### \n",
    "\n",
    "new_tokenizer.model_max_length = 8192\n",
    "\n",
    "small_model.save_pretrained(base_model + '_en_ru')\n",
    "new_tokenizer.save_pretrained(base_model + '_en_ru')\n",
    "\n",
    "# Convert to ST\n",
    "\n",
    "m = SentenceTransformer(base_model + '_en_ru')\n",
    "m.save(base_model + '_en_ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66cb1244",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:   0%|                                                                     | 0.00/1.44G [00:00<?, ?B/s]\n",
      "\n",
      "sentencepiece.bpe.model:   0%|                                                               | 0.00/1.04M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "Upload 2 LFS files:   0%|                                                                          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "sentencepiece.bpe.model: 100%|███████████████████████████████████████████████████████| 1.04M/1.04M [00:01<00:00, 732kB/s]\u001b[A\u001b[A\n",
      "model.safetensors: 100%|████████████████████████████████████████████████████████████| 1.44G/1.44G [00:41<00:00, 34.3MB/s]\n",
      "\n",
      "Upload 2 LFS files: 100%|██████████████████████████████████████████████████████████████████| 2/2 [00:42<00:00, 21.33s/it]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/TatonkaHF/bge-m3_en_ru/commit/9ca4e5b76abdd88dec49d02cf211eacb0af4cb2a'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.save_to_hub('TatonkaHF/bge-m3_en_ru', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c420f6f6",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7af2f67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name BAAI/bge-m3-unsupervised_en_ru. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "base_model = 'BAAI/bge-m3-unsupervised'\n",
    "\n",
    "e = torch.nn.modules.sparse.Embedding(\n",
    "    num_embeddings=len(resulting_vocab),\n",
    "    embedding_dim=1024,\n",
    "    padding_idx=0,\n",
    "    _weight=model_orig.embeddings.word_embeddings.weight.data[resulting_vocab, :]\n",
    ")\n",
    "\n",
    "small_model = AutoModel.from_pretrained(base_model)\n",
    "small_model.config.vocab_size = new_size\n",
    "small_model.set_input_embeddings(e)\n",
    "small_model.tie_weights()\n",
    "\n",
    "### \n",
    "\n",
    "new_tokenizer.model_max_length = 8192\n",
    "\n",
    "small_model.save_pretrained(base_model + '_en_ru')\n",
    "new_tokenizer.save_pretrained(base_model + '_en_ru')\n",
    "\n",
    "# Convert to ST\n",
    "\n",
    "m = SentenceTransformer(base_model + '_en_ru')\n",
    "m.save(base_model + '_en_ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca0ed6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "sentencepiece.bpe.model:   0%|                                                               | 0.00/1.04M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "model.safetensors:   0%|                                                             | 90.1k/1.44G [00:00<28:20, 845kB/s]\u001b[A\u001b[A\n",
      "sentencepiece.bpe.model: 100%|███████████████████████████████████████████████████████| 1.04M/1.04M [00:01<00:00, 686kB/s]\u001b[A\n",
      "model.safetensors: 100%|████████████████████████████████████████████████████████████| 1.44G/1.44G [00:42<00:00, 33.7MB/s]\n",
      "\n",
      "\n",
      "Upload 2 LFS files: 100%|██████████████████████████████████████████████████████████████████| 2/2 [00:43<00:00, 21.52s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/TatonkaHF/bge-m3-unsupervised_en_ru/commit/405b4ed72a7f1cccd8efc9e6487550ab8c7562a8'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.save_to_hub('TatonkaHF/bge-m3-unsupervised_en_ru', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "980711ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at BAAI/bge-m3-retromae and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model = 'BAAI/bge-m3-retromae'\n",
    "\n",
    "e = torch.nn.modules.sparse.Embedding(\n",
    "    num_embeddings=len(resulting_vocab),\n",
    "    embedding_dim=1024,\n",
    "    padding_idx=0,\n",
    "    _weight=model_orig.embeddings.word_embeddings.weight.data[resulting_vocab, :]\n",
    ")\n",
    "\n",
    "small_model = AutoModel.from_pretrained(base_model)\n",
    "small_model.config.vocab_size = new_size\n",
    "small_model.set_input_embeddings(e)\n",
    "small_model.tie_weights()\n",
    "\n",
    "### \n",
    "\n",
    "new_tokenizer.model_max_length = 8192\n",
    "\n",
    "small_model.save_pretrained(base_model + '_en_ru')\n",
    "new_tokenizer.save_pretrained(base_model + '_en_ru')\n",
    "\n",
    "# Convert to ST\n",
    "\n",
    "m = SentenceTransformer(base_model + '_en_ru')\n",
    "m.save(base_model + '_en_ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f92a9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:   0%|                                                                     | 0.00/1.44G [00:00<?, ?B/s]\n",
      "Upload 2 LFS files:   0%|                                                                          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "sentencepiece.bpe.model:   0%|                                                               | 0.00/1.04M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "sentencepiece.bpe.model: 100%|███████████████████████████████████████████████████████| 1.04M/1.04M [00:01<00:00, 801kB/s]\u001b[A\u001b[A\n",
      "model.safetensors: 100%|████████████████████████████████████████████████████████████| 1.44G/1.44G [00:39<00:00, 36.6MB/s]\n",
      "\n",
      "Upload 2 LFS files: 100%|██████████████████████████████████████████████████████████████████| 2/2 [00:39<00:00, 19.82s/it]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/TatonkaHF/bge-m3-retromae_en_ru/commit/3c8e2eafde4725416dd9321ff0227c80ffe6f815'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.save_to_hub('TatonkaHF/bge-m3-retromae_en_ru', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c511eddb",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0b1d544",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"baai/bge-m3\")\n",
    "model = AutoModel.from_pretrained(\"baai/bge-m3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e6c21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-m3_en_ru/\")\n",
    "small_model = AutoModel.from_pretrained(\"BAAI/bge-m3_en_ru/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a9ded38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(sentences, model, tokenizer):\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "        embeddings = model_output.pooler_output\n",
    "        embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27a90bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['мой дядя самых честных правил', 'My uncle, high ideals inspire him']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8beff942",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_new = embed(texts, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6acda54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_old = embed(texts, small_model, small_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "523279a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7131057\n",
      "0.7131057\n"
     ]
    }
   ],
   "source": [
    "print(e_new[0].dot(e_new[1]))\n",
    "print(e_old[0].dot(e_old[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214d695d",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e69e9d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.210546903744786\n",
      "0.632362277436297\n"
     ]
    }
   ],
   "source": [
    "def msize(m):\n",
    "    return sum(p.numel() for p in m.parameters())\n",
    "\n",
    "def get_sizes(model):\n",
    "    model_size = msize(model)\n",
    "    embeddings_size = msize(model.embeddings)\n",
    "    \n",
    "    return model_size, embeddings_size\n",
    "\n",
    "big_model_size, big_embeddings_size = get_sizes(model)\n",
    "small_model_size, small_embeddings_size = get_sizes(small_model)\n",
    "\n",
    "print(small_embeddings_size / big_embeddings_size)\n",
    "print(small_model_size / big_model_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
